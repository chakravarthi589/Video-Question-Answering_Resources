<h1 align="center" id="video-qa"> Video-Question-Answering (VideoQA) Resources </h1>

The Video-Question-Answering-Resources repository is a curated guide for beginners and researchers interested in the Video Question Answering (VQA) field. It provides an organized collection of the most relevant papers, models, datasets, and additional resources to help users understand and contribute to this evolving area. The repository focuses on the intersection of computer vision and natural language processing, particularly how video data can be used to answer complex questions, offering a range of materials from introductory guides to advanced research. (Last Update on 11/05/2024)

## Keywords: 
Video question answering (VideoQA), LLMs, Long video understanding, Spatial Reasoning, Temporal Reasoning, Multi-Choice QA, Open-Ended QA;

## Curators:
[ Bharatesh Chakravarthi, Ph.D](https://chakravarthi589.github.io/)
</br>
[Joseph Raj Vishal](https://github.com/joe-rabbit)

---

- [**Beginners Guide to Video-Question-Answering**](#Beginners-Guide-to-Video-Question-Answering) <br>
- [**Publications**](#Publications) <br/>
  - Survey/Review Papers
  - Conference/Journal Papers 
- [**Datasets**](#Datasets) <br>
- [**Models**](#Models) <br>
- [**Additional Resources**](#Additional-Resources) <br>

  
---
##  Beginners Guide to Video Question Answering

1. **[Answering Questions from YouTube Videos with OpenAI Whisper and GPT-4 (Medium article)](https://medium.com/@mksupriya2/answering-questions-from-youtube-videos-with-openai-whisper-and-gpt-4-9a0ae11389ba)**

2. **[Try a quick example on how to use LLMs for Video Question Answering here](https://colab.research.google.com/drive/1qTUr1rYB3L3ZlFyLocWbRKg_HVfLvyvT?usp=sharing)** (Check Additional Resources for API key)
3.  **[Community  Computer Vision Course (Unit 4) MultiModal Models](https://huggingface.co/learn/computer-vision-course/en/unit4/multimodal-models/vlm-intro)**

---
##  Publications 
### Survey/Review Papers

- A Survey on Generative AI and LLM for Video Generative Understanding, and Streaming (2024) <a href="https://arxiv.org/abs/2404.16038" target="_blank">[Paper]
- Video Question Answering: a Survey of Models and Datasets (2021) <a href="https://link.springer.com/article/10.1007/s11036-020-01730-0#ref-CR57" target="_blank">[Paper]
- A survey on VQA: Datasets and approaches (2020, ITCA) <a href="https://doi.org/10.1109/ITCA52113.2020.00069" target="_blank">[Paper]

### Conference/Journal Papers
#### 2024
- Align and Aggregate: Compositional Reasoning with Video Alignment and Answer Aggregation for Video Question Answering (**CVPR**) <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liao_Align_and_Aggregate_Compositional_Reasoning_with_Video_Alignment_and_Answer_CVPR_2024_paper.pdf" target="_blank">[Paper]
- VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models<a href="https://arxiv.org/abs/2410.00741" target="_blank">[Paper]
- MVBench: A Comprehensive Multi-modal Video Understanding Benchmark (**CVPR**)  <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_MVBench_A_Comprehensive_Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.html" target="_blank">[Paper]
- Can I Trust Your Answer? Visually Grounded Video Question Answering (**CVPR**) <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Can_I_Trust_Your_Answer_Visually_Grounded_Video_Question_Answering_CVPR_2024_paper.pdf" target="_blank">[Paper]
- MVBench: A Comprehensive Multi-modal Video Understanding Benchmark <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MVBench_A_Comprehensive_Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.pdf" target="_blank">[Paper]
- Event Graph Guided Compositional Spatial-Temporal Reasoning for Video Question Answering <a href="https://doi.org/10.1109/TIP.2024.3358726"  target="_blank">[Paper]
- LVBench: An Extreme Long Video Understanding Benchmark <a href="https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240608035W/doi:10.48550/arXiv.2406.08035" target="_blank">[Paper]
- Short Film Dataset (SFD): A Benchmark for Story-Level Video Understanding <a href="https://arxiv.org/abs/2406.10221"  target="_blank">[Paper]
- Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input <a href="https://arxiv.org/abs/2408.15542"  target="_blank">[Paper]
- CinePile: A Long Video Question Answering Dataset and Benchmark <a href="https://arxiv.org/abs/2405.08813" target="_blank">[Paper]
- Video-Language Alignment via Spatio-Temporal Graph Transformer <a href="https://arxiv.org/abs/2407.11677" target="_blank">[Paper]
- Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering <a href="https://arxiv.org/abs/2404.04007" target="_blank">[Paper]
- VideoChat: Chat-Centric Video Understanding <a href="https://arxiv.org/abs/2305.06355" target="_blank">[Paper]
- LITA: Language Instructed Temporal-Localization Assistant <a href="https://arxiv.org/html/2403.19046v1" target="_blank">[Paper]
- Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports <a href="https://arxiv.org/abs/2401.01505" target="_blank">[Paper]
- Videoagent: Long-form Video Understanding with Large Language Model as Agent <a href="https://arxiv.org/abs/2403.10517" target="_blank">[Paper]
- AMEGO: Active Memory from Long EGOcentric Videos <a href="https://arxiv.org/pdf/2409.10917" target="_blank">[Paper]
- Video Instruction Tuning With Synthetic Data <a href="https://arxiv.org/abs/2410.02713" target="_blank">[Paper]
- Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving <a href="https://arxiv.org/pdf/2403.19838" target="_blank">[Paper]
- Video-MME:The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis <a href="https://arxiv.org/pdf/2405.21075" target="_blank">[Paper]
- How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites.<a href="https://arxiv.org/pdf/2404.16821" target="_blank">[Paper]
- Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution. <a href="https://arxiv.org/pdf/2409.12191" target="_blank">[Paper]
- VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs <a href="https://arxiv.org/abs/2409.20365" target="_blank">[Paper]
- TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations <a href="https://arxiv.org/abs/2409.03206" target="_blank">[Paper]
- Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context <a href="https://arxiv.org/abs/2403.05530" target="_blank">[Paper]
- VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models <a href="https://arxiv.org/abs/2410.00741" target="_blank">[Paper]
- ViLA: Efficient video-language alignment for video question answering(ECCV24) <a href="https://www.amazon.science/publications/vila-efficient-video-language-alignment-for-video-question-answering" target="_blank">[Paper]
- STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results for Video Question Answering <a href="https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://ojs.aaai.org/index.php/AAAI/article/view/29890/31554&ved=2ahUKEwjbrNnn-LKJAxW-LUQIHYMgHKYQFnoECDEQAQ&usg=AOvVaw3fowiKyQ9-8jjR26uiZM37" target="_blank">[Paper]
- STAR: A Benchmark for Situated Reasoning in Real-World Videos <a href="https://arxiv.org/abs/2405.09711" target="_blank">[Paper]



#### 2023
- Open-Vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models (**CVPR**) <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ko_Open-vocabulary_Video_Question_Answering_A_New_Benchmark_for_Evaluating_the_ICCV_2023_paper.pdf" target="_blank">[Paper]
- ANetQA: A Large-Scale Benchmark for Fine-Grained Compositional Reasoning Over Untrimmed Videos (**CVPR**) <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_ANetQA_A_Large-Scale_Benchmark_for_Fine-Grained_Compositional_Reasoning_Over_Untrimmed_CVPR_2023_paper.pdf" target="_blank">[Paper]
- Mist: Multi-modal iterative spatial-temporal transformer for long-form video question answering (**CVPR**) <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gao_MIST_Multi-Modal_Iterative_Spatial-Temporal_Transformer_for_Long-Form_Video_Question_Answering_CVPR_2023_paper.html" target="_blank">[Paper]
- Discovering Spatio-Temporal Rationales for Video Question Answering (**ICCV**) <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Discovering_Spatio-Temporal_Rationales_for_Video_Question_Answering_ICCV_2023_paper.html" target="_blank">[Paper]
- Egoschema: A Diagnostic Benchmark for Very Long-Form Video Language Understanding (**NeurIPS**) <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/90ce332aff156b910b002ce4e6880dec-Paper-Datasets_and_Benchmarks.pdf" target="_blank">[Paper]
- Visual Instruction Tuning (**NeurIPS**) <a href="https://arxiv.org/abs/2304.08485" target="_blank">[Paper]
- A Simple LLM Framework for Long-Range Video Question-Answering (Preprint) <a href="https://arxiv.org/abs/2312.17235" target="_blank">[Paper]
- A Large Cross-Modal Video Retrieval Dataset with Reading Comprehension <a href="https://arxiv.org/abs/2305.03347">[Paper]
- InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning <a href="https://arxiv.org/abs/2305.06500" target="_blank">[Paper]
- Video Question Answering Using CLIP-Guided Visual-Text Attention <a href="https://arxiv.org/abs/2303.03131" target="_blank">[Paper]
- Building an Open-Vocabulary Video CLIP Model with Better Architectures, Optimization and Data <a href="https://arxiv.org/abs/2310.05010" target="_blank">[Paper]
- Open-VCLIP: Transforming CLIP to an Open-vocabulary Video Model via Interpolated Weight Optimization <a href="https://arxiv.org/abs/2302.00624" target="_blank">[Paper]
- Multi-Modal Correlated Network with Emotional Reasoning Knowledge for Social Intelligence Question-Answering <a href="https://openaccess.thecvf.com/content/ICCV2023W/ASI/html/Xie_Multi-Modal_Correlated_Network_with_Emotional_Reasoning_Knowledge_for_Social_Intelligence_ICCVW_2023_paper.html" target="_blank">[Paper]
- A Dataset for Medical Instructional Video Classification and Question Answering <a href="https://www.nature.com/articles/s41597-023-02036-y" target="_blank">[Paper]  

#### 2022
- Measuring Compositional Consistency for Video Question Answering (**CVPR**) <a href="https://arxiv.org/abs/2204.07190" target="_blank">[Paper]
- From Representation to Reasoning: Towards Both Evidence and Commonsense Reasoning for Video Question Answering (**CVPR**) <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_From_Representation_to_Reasoning_Towards_Both_Evidence_and_Commonsense_Reasoning_CVPR_2022_paper.pdf" target="_blank">[Paper]
- Zero-Shot Video Question Answering via Frozen Bidirectional Language Models (**NeurIPS**) <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/00d1f03b87a401b1c7957e0cc785d0bc-Paper-Conference.pdf" target="_blank">[Paper]
- Dynamic Spatio-Temporal Modular Network for Video Question Answering <a href="https://dl.acm.org/doi/10.1145/3503161.3548061" target="_blank">[Paper]
- Ego4D:Around the World in 3000 Hours of EgoCentric Video<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.pdf" target="_blank">[Paper]
- Flamingo: A Visual Language Model for Few-Shot Learning <a href="https://arxiv.org/pdf/2204.14198v2" target="_blank">[Paper]
- Saying the Unseen: Video Descriptions via Dialog Agents <a href="https://ieeexplore.ieee.org/document/9468337" target="_blank">[Paper]
- Learning to Answer Visual Questions from Web Videos <a href="https://arxiv.org/abs/2205.05019" target="_blank">[Paper]
- In-the-Wild Video Question Answering <a href="https://arxiv.org/abs/2209.06650" target="_blank">[Paper]
- FIBER:Fill-in-the-Blanks as a Challenging Video Understanding Framework <a href="https://aclanthology.org/2022.acl-long.209/" target="_blank">[Paper]
- VQuAD: Video Question Answering Diagnostic Dataset <a href="https://openaccess.thecvf.com/content/WACV2022W/DNOW/papers/Gupta_VQuAD_Video_Question_Answering_Diagnostic_Dataset_WACVW_2022_paper.pdf" target="_blank">[Paper]
- NEWSKVQA:Knowledge-Aware News Video Question Answering <a href="https://arxiv.org/abs/2202.04015" target="_blank">[Paper]
- Learning to Answer Questions in Dynamic Audio-Visual Scenarios (**CVPR**) <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.pdf" target="_blank">[Paper]


#### 2021
- NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions (**CVPR**) <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Xiao_NExT-QA_Next_Phase_of_Question-Answering_to_Explaining_Temporal_Actions_CVPR_2021_paper.pdf" target="_blank">[Paper]
- Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling (**CVPR**) <a href="https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00725" target="_blank">[Paper]
- AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning (**CVPR**) <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Grunde-McLaughlin_AGQA_A_Benchmark_for_Compositional_Spatio-Temporal_Reasoning_CVPR_2021_paper.html" target="_blank">[Paper]
- On the Hidden Treasure of Dialog in Video Question Answering (**ICCV**) <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Engin_On_the_Hidden_Treasure_of_Dialog_in_Video_Question_Answering_ICCV_2021_paper.pdf" target="_blank">[Paper]
- Self-Supervised Pre-training and Contrastive Representation Learning for Multiple-choice Video QA (**AAAI**) <a href="https://doi.org/10.1609/aaai.v35i14.17556" target="_blank">[Paper]
- Hierarchical Conditional Relation Networks for Multimodal Video Question Answering  <a href="https://link.springer.com/article/10.1007/s11263-021-01514-3" target="_blank">[Paper]
- TruMan: Trope Understanding in Movies and Animations <a href="https://doi.org/10.1145/3459637.3482018" target="_blank">[Paper]
- Perceiver IO: A General Architecture for Structured Inputs & Outputs<a href="https://arxiv.org/abs/2107.14795" target="_blank">[Paper]
- VideoGPT:Video Generation using VQ-VAE and Transformers <a href="https://arxiv.org/pdf/2104.10157" target="_blank">[Paper]
- Clip4clip: An empirical study of clip for end-to-end video clip retrieval and captioning.(ACM) <a href="https://arxiv.org/abs/2104.08860" target="_blank">[Paper]
- VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding <a href="https://arxiv.org/abs/2109.14084" target="_blank">[Paper]
- Just Ask: Learning to Answer Questions from Millions of Narrated Videos <a href="https://arxiv.org/abs/2012.00451" target="_blank">[Paper]
- AssistSR: Task-oriented Video Segment Retrieval for Personal AI Assistant <a href="https://aclanthology.org/2022.findings-emnlp.24.pdf" target="_blank">[Paper]
- SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events. (**CVPR**) <a href="" target="_blank">[Paper]
- Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments (**CVPR**)<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Env-QA_A_Video_Question_Answering_Benchmark_for_Comprehensive_Understanding_of_ICCV_2021_paper.pdf" target="_blank">[Paper]
- Progressive Graph Attention Network for Video Question Answering <a href="https://dl.acm.org/doi/10.1145/3474085.3475193" target="_blank">[Paper]
- Transferring Domain=-Agnostic Knowledge in Video Question Answering <a href="https://arxiv.org/abs/2110.13395" target="_blank">[Paper]
- Video Question Answering with Phrases via Semantic Roles <a href="https://aclanthology.org/2021.naacl-main.196/" target="_blank">[Paper]


#### 2020
- BERT Representations for Video Question Answering (**WACV**) <a href="https://openaccess.thecvf.com/content_WACV_2020/html/Yang_BERT_representations_for_Video_Question_Answering_WACV_2020_paper.html" target="_blank">[Paper]
- KnowIT VQA: Answering Knowledge-Based Questions about Videos (**AAAI**) <a href="https://doi.org/10.1609/aaai.v34i07.6713" target="_blank">[Paper]
- Divide and Conquer: Question-Guided Spatio-Temporal Contextual Attention for Video Question Answering (**AAAI**) <a href="https://doi.org/10.1609/aaai.v34i07.6766" target="_blank">[Paper]
- TVQA+: Spatio-Temporal Grounding for Video Question Answering <a href="https://aclanthology.org/2020.acl-main.730/" target="_blank">[Paper]
- Video Question Answering for Surveillance (TechRxiv - Not Peer Reviewed) <a href="https://www.techrxiv.org/users/663145/articles/675946-video-question-answering-for-surveillance" target="_blank">[Paper]
- The MSR-Video to Text Dataset with Clean Annotations <a href="https://doi.org/10.1016/j.cviu.2022.103581" target="_blank">[Paper]
- HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training (ACL) <a href="https://arxiv.org/abs/2005.00200" target="_blank">[Paper]
- CLEVRER: CoLlision Events for Video REpresentation and Reasoning <a href="https://arxiv.org/abs/1910.01442" target="_blank">[Paper]
- TVQA: Localized,Compositional Video Question Answering <a href="https://arxiv.org/abs/1809.01696" target="_blank">[Paper]
- LifeQA: A Real-Life Dataset for Video Question Answering <a href="https://aclanthology.org/2020.lrec-1.536/" target="_blank">[Paper]
- TutorialVQA: Question Answering Dataset for Tutorial Videos <a href="https://aclanthology.org/2020.lrec-1.670/" target="_blank">[Paper]
- Video Question Answering on Screencast Tutorials(ACM)<a href="https://dl.acm.org/doi/10.5555/3491440.3491588" target="_blank">[Paper]
- Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning.<a href="https://aclanthology.org/2020.emnlp-main.61/" target="_blank">[Paper]
- DramaQA: Character-Centered Video Story Understanding with Hierarchical QA.<a href="https://arxiv.org/abs/2005.03356" target="_blank">[Paper]


#### 2019
- EgoVQA: An Egocentric Video Question Answering Benchmark Dataset (**CVPR**) <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/EPIC/Fan_EgoVQA_-_An_Egocentric_Video_Question_Answering_Benchmark_Dataset_ICCVW_2019_paper.pdf" target="_blank">[Paper]
- Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering (**AAAI**) <a href="https://doi.org/10.1609/aaai.v33i01.33018658" target="_blank">[Paper]
- Compositional Attention Networks with Two-Stream Fusion for Video Question Answering <a href="https://doi.org/10.1109/TIP.2019.2940677" target="_blank">[Paper]
- Learning to Reason with Relational Video Representation for Question Answering <a href="https://www.researchgate.net/profile/Truyen-Tran-2/publication/334388370_Neural_Reasoning_Fast_and_Slow_for_Video_Question_Answering/links/5d2c386b92851cf44085055d/Neural-Reasoning-Fast-and-Slow-for-Video-Question-Answering.pdf" target="_blank">[Paper]
- Video Question Answering with Spatio-Temporal Reasoning <a href="https://link.springer.com/article/10.1007/s11263-019-01189-x" target="_blank">[Paper]
- Spatio-Temporal Relation Reasoning for Video Question Answering <a href="https://open.library.ubc.ca/media/stream/pdf/24/1.0384578/3" target="_blank">[Paper]
- Moments in Time Dataset: one million videos for event understanding <a href="http://moments.csail.mit.edu/TPAMI.2019.2901464.pdf" target="_blank">[Paper]
- Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.pdf" target="_blank">[Paper]
  
#### 2018
- Multimodal Dual Attention Memory for Video Story Question Answering (**CVPR**) <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Kyungmin_Kim_Multimodal_Dual_Attention_ECCV_2018_paper.pdf" target="_blank">[Paper]
- TVQA: Localized, Compositional Video Question Answering <a href="https://aclanthology.org/D18-1167/"  target="_blank">[Paper]
- Explore Multi-Step Reasoning in Video Question Answering <a href="https://dl.acm.org/doi/10.1145/3240508.3240563"  target="_blank">[Paper]
- Towards Automatic Learning of Procedures From Web Instructional Videos <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17344">[Paper]
- Weakly-Supervised Video Object Grounding from Text by Loss Weighting and Object Interaction <a href="http://bmvc2018.org/contents/papers/0070.pdf">[Paper]
- On the effectiveness of task granularity for transfer learning <a href="https://arxiv.org/abs/1804.09235">[Paper]

#### 2017
- A Dataset and Exploration of Models for Understanding Video Data through Fill-in-the-Blank Question Answering (**CVPR**) <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Maharaj_A_Dataset_and_CVPR_2017_paper.pdf" target="_blank">[Paper]
- MarioQA: Answering Questions by Watching Gameplay (**CVPR**) <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Mun_MarioQA_Answering_Questions_ICCV_2017_paper.pdf" target="_blank">[Paper]
- Leveraging Video Description to Learn Video Question Answering (**AAAI**) <a href="https://doi.org/10.1609/aaai.v31i1.11238" target="_blank">[Paper]
- Video Question Answering via Gradually Refined Attention over Appearance and Motion <a href="https://dl.acm.org/doi/abs/10.1145/3123266.3123427?casa_token=TPImYXpw2zYAAAAA:-yPqs_YzwkfIBEcYHzs0EAWQcprtt0HYrEugKwiYEFfNZMvZ8WqjtjJKFOX3hVwhmIvck-QCUQbHQw" target="_blank">[Paper]
- DeepStory: Video Story QA by Deep Embedded Memory Networks <a href="https://dl.acm.org/doi/10.5555/3172077.3172168" target="_blank">[Paper]
- Video Question Answering via Hierarchical Spatio-Temporal Attention Networks <a href="https://www.ijcai.org/proceedings/2017/0492.pdf" target="_blank">[Paper]
- The "something something" video database for learning and evaluating visual common sense <a href="https://arxiv.org/abs/1706.04261" target="_blank">[Paper]
- Video Question Answering via Attribute-Augmented Attention Network Learning(**ACM**) <a href="https://dl.acm.org/doi/10.1145/3077136.3080655" target="_blank">[Paper] 


#### 2016
- MovieQA: Understanding Stories in Movies through Question-Answering (**CVPR**) <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/Tapaswi_MovieQA_Understanding_Stories_CVPR_2016_paper.html" target="_blank">[Paper]
- MSR-VTT: A Large Video Description Dataset for Bridging Video and Language (**CVPR**) <a href="https://doi.org/10.1109/CVPR.2016.571" target="_blank">[Paper]
- TGIF: A New Dataset and Benchmark on Animated GIF Description (**CVPR**) <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_TGIF_A_New_CVPR_2016_paper.pdf" target="_blank">[Paper]
#### 2015
- Uncovering Temporal Context for Video Question Answering <a href="https://arxiv.org/abs/1511.04670" target="_blank">[Paper]

---
##  Datasets
| Year | Name | Key Features |
|------|------|----------|
| 2024 | [MVBench](https://github.com/OpenGVLab/Ask-Anything/blob/main/video_chat2/MVBENCH.md)| The **MVBench**  dataset focuses on evaluating multi-modal video understanding by covering 20 complex video tasks that emphasize temporal reasoning, from perception to cognition.The MVBench dataset includes over **566,747** video clips from diverse sources, such as COCO, WebVid, YouCook2, and more. The dataset also covers a wide variety of task types, such as question-answering, captioning, and conversation tasks, with more than **200** multiple-choice questions generated for each temporal understanding task |
|2024|[LVBench](https://github.com/THUDM/LVBench?tab=readme-ov-file)|The **LVBench** dataset consists of **103** videos, each with a minimum duration of **30** minutes. There are a total of **1549** question-answer pairs associated with these videos, with an average of 24 questions per hour of video content|
|2024|[MedVidQA](https://medvidqa.github.io/)|MedVidQA dataset comprises 3,010 human-annotated instructional questions and visual answers from 900 health-related videos.This dataset forms apart of the challenge of two tasks,medical instructions question generation and Video Corpus Visual Answer Localization (VCVAL).|
| 2024 | [Video-MME](https://github.com/BradyFU/Video-MME?tab=readme-ov-file) | The **Video-MME** is a comprehensive benchmark designed to evaluate Multi-Modal Large Language Models (MLLMs) in video analysis.Covers short **(< 2min)**, medium **(4-15min)**, and long **(30-60min)** videos to test MLLMs' ability to process varying time frames. This includes **6 primary domains**, such as Knowledge, Film and TV, Sports, Life Records, and Multilingualism, with **30 subfields**, ensuring broad generalizability. Integrates video frames, subtitles, and audio.|
| 2024 | [CinePile](https://ruchitrawal.github.io/cinepile/) | The **CinePile** dataset consists of **9,396 movie clips** sourced from the Movieclips YouTube channel, divided into training and testing splits of **9,248** and **148 videos**, respectively. Through a question-answer generation and filtering pipeline, the dataset produced **298,888 training points** and **4,940 test-set points**, averaging **32 questions per video scene**.|
| 2023 | [TextVR](https://github.com/callsys/TextVR) | The **TextVR** dataset is a large-scale cross-modal video retrieval dataset, containing **42,200 sentence queries** for **10,500 videos** across **eight scenario domains**, including Street View, Game, Sports, Driving, Activity, TV Show, and Cooking. |
|2023| [Social-IQ-2.0](https://github.com/CMU-MultiComp-Lab/social-iq-2.0)| This dataset is from the Social IQ challenge, consisting of 1000 videos,6000 questions and 24,000 answers. This challenge was co-hosted with the Artificial Social Intelligence Workshop at ICCV'23|
| 2023 | [VideoChat](https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data) | **VideoChat** is a video-centric multimodal instruction data based on WebVid-10M. The project features a **100K video-instruction** dataset created using  human-assisted and semi-automatic annotation techniques.|
| 2022 | [Ego4D](https://ego4d-data.org/#intro) | **Ego4D** is a comprehensive egocentric video dataset comprising **3,670 hours** of daily-life activities recorded by **931 camera wearers** across **74 locations** in **9 countries**, covering various scenarios like household, outdoor, and workplace settings.|
|2022|[NEWSKVQA](https://drive.google.com/file/d/1kK7fjwCZi0SLGGMHAENE8_Eofk_mVC15/view)|NEWSKVQA is a new dataset of 12K news videos spanning across 156 hours with 1M multiple-choice question-answer pairs covering 8263 unique entities.|
|2022| [MedVidQACL](https://github.com/deepaknlp/MedVidQACL)|This dataset consists of Medical Instructional Videos and Questions based on those videos consists of 899 Videos each of 4 mins and 3K Questions manually annotated|
|2022|[FIBER](https://github.com/MichiganNLP/video-fill-in-the-blank)|The **FIBER** dataset consists of 28,000 videos and description.The dataset consists of MCQ-type questions as well as video captioning data.Consists of 28K videos and 28K questions each of 10 seconds duration |
|2022|[Casual-VidQA](https://github.com/bcmi/Causal-VidQA)|This dataset consists of **26K** Videos with **107K** questions.Manually annotated.|
|2022|[MUSIC-AVQA](https://github.com/GeWu-Lab/MUSIC-AVQA)|This dataset consists of **9.3K** Music Video each 60s long with **45K** Manually annotated. |
|2022|[VQuAD](https://github.com/DelTA-Lab-IITK/vquad)|This dataset consists of 7K videos with 1.3Million Questions offering spatial and temporal properties.It consist of Synthetic Videos|
|2022|[STAR](https://bobbywu.com/STAR/#whatisstar)|The **STAR** is a dataset for Situated Reasoning, which provides challenging question-answering tasks, symbolic situation descriptions and logic-grounded diagnosis via real-world video situations. It consists of  4 Question Types, **60K** Situated Questions , **23K** Situation Video Clips and **140K** Situation Hypergraphs|
|2022|[In-the-Wild](https://lit.eecs.umich.edu/wildqa/)|This consists of dataset with videos recorded outdoors(survival , agriculture,natural disaster and military),Consists of **369** videos with **916** questions each about a minute and 10 seconds long.|
|2022|[AGQA 2.0](https://cs.stanford.edu/people/ranjaykrishna/agqa/)|AGQA 2.0 is the succeeding dataset of AGQA.With this  dataset, there exists a benchmark of 96.85M question-answer pairs and a balanced subset of 2.27M question-answer pairs|
|2022|[WebVidVQA3M Data](https://antoyang.github.io/just-ask.html#webvidvqa)| Consists of Web Videos **2M** with **3M** Question and each video 4 mins long.This consists of automatically tagged videos |
|2021|[HowToVQA69M Data](https://antoyang.github.io/just-ask.html)| Consists of **69M** videos, with **69M** questions each video being **2** minutes long,This consists of manually tagged videos |
|2021|[iVQA](https://antoyang.github.io/just-ask.html)|This dataset consists of **10K** videos with **10K** questions each of **8** minutes long. |
|2021|[PanoAVQA](https://github.com/HS-YN/PanoAVQA)|**PanoAVQA** dataset consists of 360 degree panoramic videos.Consists of a total of **5.4K** videos and **20K** spatial and **31.7K** audio-video QAs.|
|2021|[AGQA](https://cs.stanford.edu/people/ranjaykrishna/agqa/)|Action Genome Question Answering (AGQA) is a benchmark for compositional spatiotemporal reasoning. AGQA contains 192M unbalanced question-answer pairs for 9.6K videos. It also contains a balanced subset of 3.9M question-answer pairs |
|2021|[Video-QAP](https://github.com/TheShadow29/Video-QAP)|VideoQAP dataset consists of Web videos consisting of 35K Videos and 162K Questions each 36.2 seconds long|
|2021|[KnowIT-X-VQA](https://knowit-vqa.github.io/)|An extension of KnowIT dataset.this dataset consists of TV videos (12.1K) and **21.4K** questions.|
|2021| [Charades-SRL-QA](https://github.com/TheShadow29/Video-QAP)|These consists of Charades , HomeMade videos with **9.5K** videos with **71K** questions each 29 seconds long.|
| 2021 | [NExTQA](https://github.com/doc-doc/NExT-QA) | The **NExT-QA** dataset comprises **5,440 videos**, split into **3,870** for training, **570** for validation, and **1,000** for testing. It features around **52,044 question-answer** pairs, with approximately **47,692** for multiple-choice QA and **52,044** for open-ended QA. The questions are divided into three main types: *causal questions* (48% of the dataset), *temporal questions (29%)*, and *descriptive questions (23%)*. |
| 2021 | [LSMDC-QA](https://sites.google.com/site/describingmovies/download?authuser=0) (Requires request access) | **LSMDC-QA** (Large Scale Movie Description Challenge) contains **118,081 short video clips** extracted from **202 movies**. It consists of **7408** clips, and evaluation is performed on a test set of **1000 videos** from movies disjoint. |
|2021|[Env-QA](https://envqa.github.io/)|**Env-QA** consists of 23.3K videos collected in AI2-THOR simulator and 85.1K questions |
|2021|[SUTD-TrafficQA](https://github.com/sutdcv/SUTD-TrafficQA)|**SUTD-TrafficQA** takes the form of videoQA based dataset consists of **10080** in the wild videos and annotated **62535** QA pairs for complex traffic based scenarios.|
|2020|[CLEVRER](http://clevrer.csail.mit.edu/#Dataset)| **CLEVRER** focuses on temporal reasoning and inferencing of synthetic videos. Consists of **10K** videos **305K** Questions each of 5 seconds duration |
|2020| [LifeQA](https://github.com/mmazab/LifeQA)| **LifeQA** consists of videos of data-to-data activities.It consists **275**  video clips and over **2.3k** multiple-choice questions.|
|2020|[How2R-and-How2QA](https://github.com/ych133/How2R-and-How2QA)|The **How2R and How2QA** datasets contain **9,371** and **9,035** episodes, with **24,328** and **21,509** clips averaging around **17** seconds each, divided into training, validation, and testing sets.|
|2021|[TGIF-QA-R](https://github.com/PengLiang-cn/PGAT)|This dataset consists of **71K** GIFs each of **3** seconds long and **165K** Questions.Extended version of the TGIF-QA dataset. |
|2020|[DramaQA](https://dramaqa.snu.ac.kr/)|**DramaQA** dataset is built upon the TV drama "Another Miss Oh" and it contains 17,983 QA pairs from 23,928 various-length video clips, with each QA pair belonging to one of four difficulty levels. |
|2020|[KnowITVQA](https://knowit-vqa.github.io/)|**KnowITVQA** is a video dataset with 24,282 human-generated question-answer pairs about The Big Bang Theory consisting of **207** videos each 20 minutes long |
|2020|[V2C-QA](https://github.com/jacobswan1/Video2Commonsense)|Video2CommonSense dataset consists of Web Videos for image captioning and VideoQA consists of **1.5K** videos and **37K** questions.|
| 2020|[PsTuts-VQA](https://github.com/adobe-research/PsTuts-VQA-Dataset)|The **PsTuts** dataset includes the following resources: 76 videos (5.6 hours in total), 17,768 question-answer pairs, and a domain knowledge-base with 1,236 entities and 2,196 options.It focuses on video tutorials|
| 2019 | [Social-QA](https://www.kaggle.com/datasets/mathurinache/social-iq?select=Social-IQ) | This Kaggle repository consists of the Social-QA dataset. **Social-IQ** contains **1,250** natural in-the-wild social situations, **7,500** questions and **52,500** correct and incorrect answers.| 
| 2019 |[TutorialVQAD](https://github.com/acolas1/TutorialVQAData)| **TutorialVQAD** consists of tutorial pertaining to image editing software. Total number of videos **76** and total number of questions **6195** | 
|2019 | [Moments in Time Dataset](http://moments.csail.mit.edu/#) | **The Moments in Time dataset** consists of one million videos, each 3 seconds long, with 339 different classes.  |
| 2018 | [TVQA](https://github.com/jayleicn/TVQA/tree/master?tab=readme-ov-file) | **TVQA** is a large-scale video question-answering dataset built from six popular TV shows, including *Friends*, *The Big Bang Theory*, and *How I Met Your Mother*. It contains **152.5K QA** pairs sourced from **21.8K video clips**, covering over **460 hours** of content.|
| 2018 | [SVQA](https://svqa-founder.github.io/SVQA/) | **SVQA** dataset consists of Attribute comparison, count, integer comparison, exist and query type questions.This consists of synthetic videos almost **12K** and **118K** Questions |
| 2018 | [YouCook2](http://youcook2.eecs.umich.edu/) | **YouCook2** is one of the largest instructional video datasets focused on task-oriented cooking, featuring **2,000 untrimmed videos** from **89 recipes**, with an average of **22 videos** per recipe. Each video, averaging **5.26 minutes** and totalling **176 hours**, includes annotated procedure steps with their corresponding temporal boundaries. |
| 2018 | [TVQA+](https://github.com/jayleicn/TVQAplus) | **TVQA+** includes **29.4K multiple-choice questions** grounded in both temporal and spatial domains. A set of visual concept words—objects and people—are identified to collect spatial groundings, and corresponding object regions in individual frames are annotated with bounding boxes.  |
| 2017 | [TGIF-QA](https://github.com/YunseokJANG/tgif-qa) | **TGIF-QA**, a large-scale dataset, contains **165K question-answer** pairs based on animated GIFs, testing video-based Visual Question Answering (VQA) across four question types: Repetition Count, Repeating Action, State Transition, and Frame QA.|
| 2017 | [MarioQA](https://github.com/JonghwanMun/MarioQA) | **MarioQA** is a dataset specifically designed for video-based question-answering in the context of *Super Mario Bros.* gameplay, containing over **70,000 question-answer pairs** linked to gameplay footage.|
|2017| [VideoQA](https://aliensunmin.github.io/project/video-language/)| **VideoQA** dataset of **18100** automatically crawled user-generated videos and titles.Videos collected from web videos with **174k** questions each of 90s each |
| 2017 | [Something-Something v1 & v2](https://www.qualcomm.com/developer/software/something-something-v-2-dataset) | **Something-Something** is a collection of 220,847 labelled video clips of humans performing predefined basic actions with everyday objects. The dataset comprises **220,847 videos** divided into a training set of **168,913**, a validation set of **24,777**, and a test set of **27,157 (without labels)**, totalling **174 unique labels**.|
| 2016 | [MSVD-QA](https://github.com/xudejing/video-question-answering?tab=readme-ov-file) | The **MSVD-QA** dataset is a Video Question Answering (VideoQA) dataset derived from the **Microsoft Research Video Description (MSVD)** dataset, which includes around **120K sentences** describing over **2,000 videos** snippets. The dataset includes **1,970 video clips** and approximately **50.5K QA pairs**. |
| 2016 | [MSRVTT-QA](https://github.com/xudejing/video-question-answering?tab=readme-ov-file) | **MSRVTT-QA** consists of **10K web video clips** with a total duration of **41.2 hours**. It spans **200k clip-sentence pairs**. Each video clip is annotated with about **20 natural sentences.** |
| 2016 | [MovieQA](https://github.com/makarandtapaswi/MovieQA_benchmark?tab=readme-ov-file) | The **MovieQA dataset** is designed for movie question answering, aimed at evaluating automatic story comprehension through both video and text. It contains nearly **15,000 multiple-choice questions** derived from over **400 movies**.|
| 2016 | [PororoQA](https://github.com/Kyung-Min/PororoQA) | The **Pororo** dataset based on children's cartoons features a simple story structure with episodes averaging **7.2 minutes**, where similar events are frequently repeated. The dataset comprises **8,834 QA pairs**, with an average of **51.66 questions per episode**, excluding ambiguous or unrelated questions. |
| 2015 |[VideoQA(FIB)](https://github.com/ffmpbgrnn/VideoQA)|This dataset consists of VideoQA, from multiple sources with videdos **109K** video clips and duration of over 1000 hours with **390744** questions. |
| 2014 | [Activity Net](http://activity-net.org/download.html) | **ActivityNet** is a large-scale video benchmark for human activity understanding. ActivityNet aims to cover a wide range of complex human activities. ActivityNet provides samples from **203 activity classes** with an average of **137 untrimmed videos** per class and **1.41 activity instances** per video, for a total of **849 video hours**. |
|2013 | [YouTube2Text-QA](https://openaccess.thecvf.com/content_iccv_2013/papers/Guadarrama_YouTube2Text_Recognizing_and_2013_ICCV_paper.pdf)| **YouTube2Text** data consists of **1987** videos with **122708** descriptions.These include short descriptions of videos.|
---
##  Models
## Open Source Models
| Model Name  | Links |
|-------------|-------------------------------|
| InternVL | [Hugging Face](https://huggingface.co/OpenGVLab/InternVL2-76B) , [GitHub](https://github.com/OpenGVLab/InternVL) |
| LLaVa | [Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/llava) , [GitHub](https://github.com/haotian-liu/LLaVA) |
| LITA | [GitHub](https://github.com/NVlabs/LITA)|
| End2End ChatBot |[Hugging Face](https://huggingface.co/spaces/OpenGVLab/InternVideo2-Chat-8B-HD) , [GitHub](https://github.com/OpenGVLab/Ask-Anything)|
| VideoLLAMA2 | [Hugging Face](https://huggingface.co/spaces/lixin4ever/VideoLLaMA2), [GitHub](https://github.com/DAMO-NLP-SG/VideoLLaMA2) |
|FrozenBiLM | [GitHub](https://github.com/antoyang/FrozenBiLM) |
|PercieverIO | [Hugging Face](https://huggingface.co/docs/transformers/model_doc/perceiver),[GitHub](https://github.com/google-deepmind/deepmind-research/tree/master/perceiver)|
|InstructBlipVideo |[Hugging Face](https://huggingface.co/docs/transformers/model_doc/instructblipvideo#instructblipvideo) , [GitHub](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) |
|VideoGPT|[Hugging Face](https://huggingface.co/notbdq/videogpt),[GitHub](https://github.com/wilson1yan/VideoGPT?tab=readme-ov-file)|
|Qwen2-VL|[Hugging Face](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d),[GitHub](https://github.com/QwenLM/Qwen2-VL?tab=readme-ov-file)|
|ViLA|[GitHub](https://www.amazon.science/publications/vila-efficient-video-language-alignment-for-video-question-answering)|

---

## Closed Source Models
| Model Name | API Link |
|------------|----------|
| ChatGPT    | [Here](https://platform.openai.com/api-keys) |
| Gemini |[Here](https://ai.google.dev/gemini-api/docs/vision?lang=python)|
| Llama 3.2|[Here](https://docs.llama-api.com/quickstart#llama-3-2-instruct-chat-models-with-vision)|


---

##  Additional-Resources

1. **[OpenAI Docs](https://platform.openai.com/docs/api-reference/introduction)**
2. **[Gemini Docs](https://ai.google.dev/gemini-api/docs)**
3. **[LLAMA Docs](https://docs.llama-api.com/quickstart)**
4. **[Azure Samples](https://github.com/Azure-Samples/azure-video-indexer-samples/tree/master/VideoQnA-Demo)**
---

### :arrow_heading_up: [Back to Top](#video-qa)





